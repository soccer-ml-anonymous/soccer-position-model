# -*- coding: utf-8 -*-
"""ポジションの判別（教師あり学習）2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1atx6wMqQdPFQD4STPy9t0RTFjS_wG_nF

**★ライブラリのインポート**
"""

import numpy as np
import tensorflow as tf
import glob
import cv2
import os

"""**★画像の準備**

**【参考：事前実施済】1）51クラスタの画像を14クラスタに並べ替える**
"""

# Googleドライブのマウント設定（左側のフォルダマーククリックで解決？）
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import shutil
import re

# Google Drive 上のパス（必要に応じて変更してください）
source_base_folder = '/content/drive/MyDrive/heatmaps_51クラスタ'
base_folder = '/content/drive/MyDrive/heatmaps_14クラスタ'
excel_path = '/content/drive/MyDrive/clustering_results_14_split.xlsx'

# Excelファイル読み込み（最初のシート）
df = pd.read_excel(excel_path)

# クラスタマッピング作成（match_id + player_id → cluster番号）
cluster_map = {}
for index, row in df.iterrows():
    match_id = str(row['Match_ID']).replace('M', '')
    player_id = str(row['Player_ID']).replace('P', '')
    cluster_str = str(row['14Cluster'])
    match = re.match(r'\d+', cluster_str)
    if match:
        cluster_num = int(match.group())
        cluster_label = f"cluster_{cluster_num}"
        key = f"{match_id}_{player_id}"
        cluster_map[key] = cluster_label

# NEW15クラスタのフォルダ作成
os.makedirs(base_folder, exist_ok=True)
for i in range(1, 14):
    os.makedirs(os.path.join(base_folder, f'cluster_{i}'), exist_ok=True)

# 画像ファイルを移動
for i in range(1, 52):
    folder_path = os.path.join(source_base_folder, f'cluster_{i}')
    if not os.path.exists(folder_path):
        continue
    for file in os.listdir(folder_path):
        if file.endswith('.png'):
            parts = file.split('_')
            if len(parts) >= 6:
                match_id = parts[3]
                player_id = parts[5].split('.')[0]
                key = f"{match_id}_{player_id}"
                if key in cluster_map:
                    target_cluster = cluster_map[key]
                    target_path = os.path.join(base_folder, target_cluster, file)
                    print(f"Moving file: {file} to {target_cluster}")
                    shutil.copy(os.path.join(folder_path, file), target_path)

print("✅ すべてのファイルの移動が完了しました。")

"""**2）画像をtrain,testの2群に振り分けデータ行列化する**

【確認】2群に分けたcsvファイルを確認
"""

import os
print(os.path.exists('/content/drive/MyDrive/clustering_results_14_split.csv'))

"""**実行**"""

import pandas as pd
import numpy as np
import tensorflow as tf
import glob
import re
import os

# Google Drive 上のパス
base_folder = '/content/drive/MyDrive/heatmaps_14クラスタ'
csv_path = '/content/drive/MyDrive/clustering_results_14_split.csv'

# CSVファイル読み込み
df = pd.read_csv(csv_path)

# データセットの初期化
X_train, y_train = [], []
X_test, y_test = [], []

# クラスタマッピング作成（match_id + player_id → set, cluster）
set_map = {}
for index, row in df.iterrows():
    match_id = str(row['Match_ID']).replace('M', '')
    player_id = str(row['Player_ID']).replace('P', '')
    set_label = row['set']

    match = re.match(r'\d+', str(row['14Cluster']))
    if match:
        cluster_label = int(match.group())
        key = f"{match_id}_{player_id}"
        set_map[key] = (set_label, cluster_label)
    else:
        print(f"[CSV] Skipping row {index}: invalid cluster label '{row['14Cluster']}'")

print(f"Total keys in set_map: {len(set_map)}")
print("Sample keys from set_map:", list(set_map.keys())[:5])

# 画像ファイルを読み込み、train/testに分類
image_files = glob.glob(os.path.join(base_folder, 'cluster_*', 'pass_heatmap_match_*.png'))
print(f"Total image files found: {len(image_files)}")

for f in image_files:
    parts = os.path.basename(f).split('_')
    if len(parts) < 6:
        print(f"[SKIP] Unexpected filename format: {f}")
        continue

    match_id = parts[3].replace('M', '')
    player_id = parts[5].replace('.png', '').replace('.0', '').replace('P', '')
    key = f"{match_id}_{player_id}"

    if key not in set_map:
        print(f"[SKIP] Key not found in set_map: {key} (from file: {f})")
        continue

    try:
        img_data = tf.io.read_file(f)
        img_data = tf.io.decode_png(img_data, channels=3)
        img_data = tf.image.resize(img_data, [224, 224])
        img_data = img_data.numpy()

        set_label, cluster_label = set_map[key]
        if set_label == 1:
            X_train.append(img_data)
            y_train.append(cluster_label)
        elif set_label == 2:
            X_test.append(img_data)
            y_test.append(cluster_label)
    except Exception as e:
        print(f"[ERROR] Failed to load image {f}: {e}")

# 正規化
X_train = np.array(X_train) / 255.0 if X_train else np.array([])
y_train = np.array(y_train)
X_test = np.array(X_test) / 255.0 if X_test else np.array([])
y_test = np.array(y_test)

# データセットの形状を表示
print(f"\nX_train shape: {X_train.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_test shape: {y_test.shape}")

"""【参考】25年1月に実施した元コード"""

X_train = []
y_train = []
X_test = []
y_test = []

for f in glob.glob("image/*/*/*.png"):
  img_data = tf.io.read_file(f)
  img_data = tf.io.decode_png(img_data)
  img_data = tf.image.resize(img_data,[100,100])
  img_data = img_data.numpy()

  if f.split("/")[1] == "train":
      X_train.append(img_data)
      y_train.append(int(f.split("/")[2].split("_")[0]))
  elif f.split("/")[1] == "test":
      X_test.append(img_data)
      y_test.append(int(f.split("/")[2].split("_")[0]))

X_train = np.array(X_train) / 255.0
y_train = np.array(y_train)
X_test = np.array(X_test) / 255.0
y_test = np.array(y_test)

"""**★ニューラルネットワークの構築，　モデルの学習**

**新_CNNの構築, 学習, 評価**
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc

# クラス名（14クラス）
target_names = [
    "1GK", "2LFB", "3LCB", "4LWB", "5LWG", "6RCMF", "7RCB", "8RWB",
    "9RWG", "10LCM", "11CMF", "12SS", "13ST", "14RFB"
]

# クラス数を自動で取得
num_classes = np.max(y_train) + 1
print("Number of classes:", num_classes)

# モデル定義
model = tf.keras.models.Sequential([
    tf.keras.Input(shape=(224, 224, 3)),
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(num_classes, activation='softmax')
])

# コンパイル
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])

# 学習（履歴を保存）
history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)

# 最良エポックを特定
best_epoch = np.argmax(history.history['val_accuracy']) + 1
print(f"Best epoch based on validation accuracy: {best_epoch}")

# モデルを再構築して最良エポックまで再学習
model = tf.keras.models.clone_model(model)
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
model.fit(X_train, y_train, epochs=best_epoch, validation_data=(X_test, y_test), batch_size=32)

# 予測と評価
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# 分類レポート
print("📊 Classification Report:")
report = classification_report(y_test, y_pred_classes, target_names=target_names, output_dict=True)
for label, metrics in report.items():
    if isinstance(metrics, dict):
        print(f"{label}: Precision={metrics['precision']:.2f}, Recall={metrics['recall']:.2f}, F1-score={metrics['f1-score']:.2f}")

# 混同行列
conf_matrix = confusion_matrix(y_test, y_pred_classes)
print("\n🧩 Confusion Matrix:")
print(conf_matrix)

# クラスごとの正解率
print("\n🎯 Accuracy per class:")
for i in range(conf_matrix.shape[0]):
    correct = conf_matrix[i, i]
    total = np.sum(conf_matrix[i, :])
    acc = correct / total if total > 0 else 0
    print(f"{target_names[i]}: Accuracy = {acc:.2f}")

# 特異度と偽陽性率
print("\n🔍 Specificity and 🚫 False Positive Rate per class:")
for i in range(conf_matrix.shape[0]):
    tn = np.sum(np.delete(np.delete(conf_matrix, i, axis=0), i, axis=1))
    fp = np.sum(np.delete(conf_matrix[i, :], i))
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
    print(f"{target_names[i]}: Specificity = {specificity:.2f}, FPR = {fpr:.2f}")

# ROC AUC曲線
plt.figure(figsize=(10, 8))
for i in range(num_classes):
    fpr, tpr, _ = roc_curve(y_test == i, y_pred[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{target_names[i]} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC AUC Curves')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

"""旧コード　CNN構築と学習"""

import tensorflow as tf
import numpy as np

# クラス数を自動で取得（例：0〜15 → 16クラス）
num_classes = np.max(y_train) + 1
print("Number of classes:", num_classes)

# モデル定義：CNN(畳み込みニューラルネットワーク)←推奨
model = tf.keras.models.Sequential([
    tf.keras.Input(shape=(224, 224, 3)),
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(num_classes, activation='softmax')
])


# コンパイル
model.compile(optimizer="adam",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])

# 学習
model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))

"""旧コード. モデルの評価"""

from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np

# モデルの再定義
best_epoch = np.argmax(history.history['val_accuracy']) + 1
print(f"Best epoch based on validation accuracy: {best_epoch}")

history = model.fit(
    X_train, y_train,
    epochs=best_epoch,# 例：val_accuracyが最も高かったエポック数
    validation_data=(X_test, y_test),
    batch_size=32
)

# 予測
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# 基本指標
target_names = [
    "1GK", "2LFB", "3LCB", "4LWB", "5LWG", "6RCMF", "7RCB", "8RWB",
    "9RWG", "10LCM", "11CMF", "12SS", "13ST", "14LRWG", "15RFB", "16UNK"
]

print("📊 Classification Report:")
report = classification_report(y_test, y_pred_classes, target_names=target_names, output_dict=True)
for label, metrics in report.items():
    if isinstance(metrics, dict):
        print(f"Class {label}: Precision={metrics['precision']:.2f}, Recall={metrics['recall']:.2f}, F1-score={metrics['f1-score']:.2f}")

# 混同行列
conf_matrix = confusion_matrix(y_test, y_pred_classes)
print("\n🧩 Confusion Matrix:")
print(conf_matrix)

# クラスタごとの正解率（Accuracy）を計算
class_accuracy = {}
for i in range(conf_matrix.shape[0]):
    correct = conf_matrix[i, i]
    total = np.sum(conf_matrix[i, :])
    class_accuracy[i] = correct / total
print("\n🎯 Accuracy per class:")
for class_label, accuracy in class_accuracy.items():
    print(f"Class {class_label}: Accuracy = {accuracy:.2f}")

# 特異度と偽陽性率
specificity = {}
false_positive_rate = {}
for i in range(conf_matrix.shape[0]):
    tn = np.sum(np.delete(np.delete(conf_matrix, i, axis=0), i, axis=1))
    fp = np.sum(np.delete(conf_matrix[i, :], i))
    specificity[i] = tn / (tn + fp)
    false_positive_rate[i] = fp / (fp + tn)

print("\n🔍 Specificity per class:")
print(specificity)
print("\n🚫 False Positive Rate per class:")
print(false_positive_rate)

# ROC AUC曲線
plt.figure(figsize=(10, 8))
for i in range(np.max(y_test)+1):
    fpr, tpr, _ = roc_curve(y_test == i, y_pred[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC AUC Curves')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

model.evaluate(X_test,y_test)

np.argmax(model.predict(X_test))

"""***※工程の確認↓***"""

