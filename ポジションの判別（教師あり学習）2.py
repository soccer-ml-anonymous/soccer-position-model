# -*- coding: utf-8 -*-
"""ãƒã‚¸ã‚·ãƒ§ãƒ³ã®åˆ¤åˆ¥ï¼ˆæ•™å¸«ã‚ã‚Šå­¦ç¿’ï¼‰2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1atx6wMqQdPFQD4STPy9t0RTFjS_wG_nF

**â˜…ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ**
"""

import numpy as np
import tensorflow as tf
import glob
import cv2
import os

"""**â˜…ç”»åƒã®æº–å‚™**

**ã€å‚è€ƒï¼šäº‹å‰å®Ÿæ–½æ¸ˆã€‘1ï¼‰51ã‚¯ãƒ©ã‚¹ã‚¿ã®ç”»åƒã‚’14ã‚¯ãƒ©ã‚¹ã‚¿ã«ä¸¦ã¹æ›¿ãˆã‚‹**
"""

# Googleãƒ‰ãƒ©ã‚¤ãƒ–ã®ãƒã‚¦ãƒ³ãƒˆè¨­å®šï¼ˆå·¦å´ã®ãƒ•ã‚©ãƒ«ãƒ€ãƒãƒ¼ã‚¯ã‚¯ãƒªãƒƒã‚¯ã§è§£æ±ºï¼Ÿï¼‰
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import shutil
import re

# Google Drive ä¸Šã®ãƒ‘ã‚¹ï¼ˆå¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ã—ã¦ãã ã•ã„ï¼‰
source_base_folder = '/content/drive/MyDrive/heatmaps_51ã‚¯ãƒ©ã‚¹ã‚¿'
base_folder = '/content/drive/MyDrive/heatmaps_14ã‚¯ãƒ©ã‚¹ã‚¿'
excel_path = '/content/drive/MyDrive/clustering_results_14_split.xlsx'

# Excelãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆæœ€åˆã®ã‚·ãƒ¼ãƒˆï¼‰
df = pd.read_excel(excel_path)

# ã‚¯ãƒ©ã‚¹ã‚¿ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆï¼ˆmatch_id + player_id â†’ clusterç•ªå·ï¼‰
cluster_map = {}
for index, row in df.iterrows():
    match_id = str(row['Match_ID']).replace('M', '')
    player_id = str(row['Player_ID']).replace('P', '')
    cluster_str = str(row['14Cluster'])
    match = re.match(r'\d+', cluster_str)
    if match:
        cluster_num = int(match.group())
        cluster_label = f"cluster_{cluster_num}"
        key = f"{match_id}_{player_id}"
        cluster_map[key] = cluster_label

# NEW15ã‚¯ãƒ©ã‚¹ã‚¿ã®ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
os.makedirs(base_folder, exist_ok=True)
for i in range(1, 14):
    os.makedirs(os.path.join(base_folder, f'cluster_{i}'), exist_ok=True)

# ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç§»å‹•
for i in range(1, 52):
    folder_path = os.path.join(source_base_folder, f'cluster_{i}')
    if not os.path.exists(folder_path):
        continue
    for file in os.listdir(folder_path):
        if file.endswith('.png'):
            parts = file.split('_')
            if len(parts) >= 6:
                match_id = parts[3]
                player_id = parts[5].split('.')[0]
                key = f"{match_id}_{player_id}"
                if key in cluster_map:
                    target_cluster = cluster_map[key]
                    target_path = os.path.join(base_folder, target_cluster, file)
                    print(f"Moving file: {file} to {target_cluster}")
                    shutil.copy(os.path.join(folder_path, file), target_path)

print("âœ… ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ç§»å‹•ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

"""**2ï¼‰ç”»åƒã‚’train,testã®2ç¾¤ã«æŒ¯ã‚Šåˆ†ã‘ãƒ‡ãƒ¼ã‚¿è¡Œåˆ—åŒ–ã™ã‚‹**

ã€ç¢ºèªã€‘2ç¾¤ã«åˆ†ã‘ãŸcsvãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
"""

import os
print(os.path.exists('/content/drive/MyDrive/clustering_results_14_split.csv'))

"""**å®Ÿè¡Œ**"""

import pandas as pd
import numpy as np
import tensorflow as tf
import glob
import re
import os

# Google Drive ä¸Šã®ãƒ‘ã‚¹
base_folder = '/content/drive/MyDrive/heatmaps_14ã‚¯ãƒ©ã‚¹ã‚¿'
csv_path = '/content/drive/MyDrive/clustering_results_14_split.csv'

# CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
df = pd.read_csv(csv_path)

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆæœŸåŒ–
X_train, y_train = [], []
X_test, y_test = [], []

# ã‚¯ãƒ©ã‚¹ã‚¿ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆï¼ˆmatch_id + player_id â†’ set, clusterï¼‰
set_map = {}
for index, row in df.iterrows():
    match_id = str(row['Match_ID']).replace('M', '')
    player_id = str(row['Player_ID']).replace('P', '')
    set_label = row['set']

    match = re.match(r'\d+', str(row['14Cluster']))
    if match:
        cluster_label = int(match.group())
        key = f"{match_id}_{player_id}"
        set_map[key] = (set_label, cluster_label)
    else:
        print(f"[CSV] Skipping row {index}: invalid cluster label '{row['14Cluster']}'")

print(f"Total keys in set_map: {len(set_map)}")
print("Sample keys from set_map:", list(set_map.keys())[:5])

# ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€train/testã«åˆ†é¡
image_files = glob.glob(os.path.join(base_folder, 'cluster_*', 'pass_heatmap_match_*.png'))
print(f"Total image files found: {len(image_files)}")

for f in image_files:
    parts = os.path.basename(f).split('_')
    if len(parts) < 6:
        print(f"[SKIP] Unexpected filename format: {f}")
        continue

    match_id = parts[3].replace('M', '')
    player_id = parts[5].replace('.png', '').replace('.0', '').replace('P', '')
    key = f"{match_id}_{player_id}"

    if key not in set_map:
        print(f"[SKIP] Key not found in set_map: {key} (from file: {f})")
        continue

    try:
        img_data = tf.io.read_file(f)
        img_data = tf.io.decode_png(img_data, channels=3)
        img_data = tf.image.resize(img_data, [224, 224])
        img_data = img_data.numpy()

        set_label, cluster_label = set_map[key]
        if set_label == 1:
            X_train.append(img_data)
            y_train.append(cluster_label)
        elif set_label == 2:
            X_test.append(img_data)
            y_test.append(cluster_label)
    except Exception as e:
        print(f"[ERROR] Failed to load image {f}: {e}")

# æ­£è¦åŒ–
X_train = np.array(X_train) / 255.0 if X_train else np.array([])
y_train = np.array(y_train)
X_test = np.array(X_test) / 255.0 if X_test else np.array([])
y_test = np.array(y_test)

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å½¢çŠ¶ã‚’è¡¨ç¤º
print(f"\nX_train shape: {X_train.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_test shape: {y_test.shape}")

"""ã€å‚è€ƒã€‘25å¹´1æœˆã«å®Ÿæ–½ã—ãŸå…ƒã‚³ãƒ¼ãƒ‰"""

X_train = []
y_train = []
X_test = []
y_test = []

for f in glob.glob("image/*/*/*.png"):
  img_data = tf.io.read_file(f)
  img_data = tf.io.decode_png(img_data)
  img_data = tf.image.resize(img_data,[100,100])
  img_data = img_data.numpy()

  if f.split("/")[1] == "train":
      X_train.append(img_data)
      y_train.append(int(f.split("/")[2].split("_")[0]))
  elif f.split("/")[1] == "test":
      X_test.append(img_data)
      y_test.append(int(f.split("/")[2].split("_")[0]))

X_train = np.array(X_train) / 255.0
y_train = np.array(y_train)
X_test = np.array(X_test) / 255.0
y_test = np.array(y_test)

"""**â˜…ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ§‹ç¯‰ï¼Œã€€ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’**

**æ–°_CNNã®æ§‹ç¯‰, å­¦ç¿’, è©•ä¾¡**
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc

# ã‚¯ãƒ©ã‚¹åï¼ˆ14ã‚¯ãƒ©ã‚¹ï¼‰
target_names = [
    "1GK", "2LFB", "3LCB", "4LWB", "5LWG", "6RCMF", "7RCB", "8RWB",
    "9RWG", "10LCM", "11CMF", "12SS", "13ST", "14RFB"
]

# ã‚¯ãƒ©ã‚¹æ•°ã‚’è‡ªå‹•ã§å–å¾—
num_classes = np.max(y_train) + 1
print("Number of classes:", num_classes)

# ãƒ¢ãƒ‡ãƒ«å®šç¾©
model = tf.keras.models.Sequential([
    tf.keras.Input(shape=(224, 224, 3)),
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(num_classes, activation='softmax')
])

# ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])

# å­¦ç¿’ï¼ˆå±¥æ­´ã‚’ä¿å­˜ï¼‰
history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)

# æœ€è‰¯ã‚¨ãƒãƒƒã‚¯ã‚’ç‰¹å®š
best_epoch = np.argmax(history.history['val_accuracy']) + 1
print(f"Best epoch based on validation accuracy: {best_epoch}")

# ãƒ¢ãƒ‡ãƒ«ã‚’å†æ§‹ç¯‰ã—ã¦æœ€è‰¯ã‚¨ãƒãƒƒã‚¯ã¾ã§å†å­¦ç¿’
model = tf.keras.models.clone_model(model)
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
model.fit(X_train, y_train, epochs=best_epoch, validation_data=(X_test, y_test), batch_size=32)

# äºˆæ¸¬ã¨è©•ä¾¡
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
print("ğŸ“Š Classification Report:")
report = classification_report(y_test, y_pred_classes, target_names=target_names, output_dict=True)
for label, metrics in report.items():
    if isinstance(metrics, dict):
        print(f"{label}: Precision={metrics['precision']:.2f}, Recall={metrics['recall']:.2f}, F1-score={metrics['f1-score']:.2f}")

# æ··åŒè¡Œåˆ—
conf_matrix = confusion_matrix(y_test, y_pred_classes)
print("\nğŸ§© Confusion Matrix:")
print(conf_matrix)

# ã‚¯ãƒ©ã‚¹ã”ã¨ã®æ­£è§£ç‡
print("\nğŸ¯ Accuracy per class:")
for i in range(conf_matrix.shape[0]):
    correct = conf_matrix[i, i]
    total = np.sum(conf_matrix[i, :])
    acc = correct / total if total > 0 else 0
    print(f"{target_names[i]}: Accuracy = {acc:.2f}")

# ç‰¹ç•°åº¦ã¨å½é™½æ€§ç‡
print("\nğŸ” Specificity and ğŸš« False Positive Rate per class:")
for i in range(conf_matrix.shape[0]):
    tn = np.sum(np.delete(np.delete(conf_matrix, i, axis=0), i, axis=1))
    fp = np.sum(np.delete(conf_matrix[i, :], i))
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
    print(f"{target_names[i]}: Specificity = {specificity:.2f}, FPR = {fpr:.2f}")

# ROC AUCæ›²ç·š
plt.figure(figsize=(10, 8))
for i in range(num_classes):
    fpr, tpr, _ = roc_curve(y_test == i, y_pred[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{target_names[i]} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC AUC Curves')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

"""æ—§ã‚³ãƒ¼ãƒ‰ã€€CNNæ§‹ç¯‰ã¨å­¦ç¿’"""

import tensorflow as tf
import numpy as np

# ã‚¯ãƒ©ã‚¹æ•°ã‚’è‡ªå‹•ã§å–å¾—ï¼ˆä¾‹ï¼š0ã€œ15 â†’ 16ã‚¯ãƒ©ã‚¹ï¼‰
num_classes = np.max(y_train) + 1
print("Number of classes:", num_classes)

# ãƒ¢ãƒ‡ãƒ«å®šç¾©ï¼šCNN(ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯)â†æ¨å¥¨
model = tf.keras.models.Sequential([
    tf.keras.Input(shape=(224, 224, 3)),
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(num_classes, activation='softmax')
])


# ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
model.compile(optimizer="adam",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])

# å­¦ç¿’
model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))

"""æ—§ã‚³ãƒ¼ãƒ‰. ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡"""

from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np

# ãƒ¢ãƒ‡ãƒ«ã®å†å®šç¾©
best_epoch = np.argmax(history.history['val_accuracy']) + 1
print(f"Best epoch based on validation accuracy: {best_epoch}")

history = model.fit(
    X_train, y_train,
    epochs=best_epoch,# ä¾‹ï¼šval_accuracyãŒæœ€ã‚‚é«˜ã‹ã£ãŸã‚¨ãƒãƒƒã‚¯æ•°
    validation_data=(X_test, y_test),
    batch_size=32
)

# äºˆæ¸¬
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# åŸºæœ¬æŒ‡æ¨™
target_names = [
    "1GK", "2LFB", "3LCB", "4LWB", "5LWG", "6RCMF", "7RCB", "8RWB",
    "9RWG", "10LCM", "11CMF", "12SS", "13ST", "14LRWG", "15RFB", "16UNK"
]

print("ğŸ“Š Classification Report:")
report = classification_report(y_test, y_pred_classes, target_names=target_names, output_dict=True)
for label, metrics in report.items():
    if isinstance(metrics, dict):
        print(f"Class {label}: Precision={metrics['precision']:.2f}, Recall={metrics['recall']:.2f}, F1-score={metrics['f1-score']:.2f}")

# æ··åŒè¡Œåˆ—
conf_matrix = confusion_matrix(y_test, y_pred_classes)
print("\nğŸ§© Confusion Matrix:")
print(conf_matrix)

# ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®æ­£è§£ç‡ï¼ˆAccuracyï¼‰ã‚’è¨ˆç®—
class_accuracy = {}
for i in range(conf_matrix.shape[0]):
    correct = conf_matrix[i, i]
    total = np.sum(conf_matrix[i, :])
    class_accuracy[i] = correct / total
print("\nğŸ¯ Accuracy per class:")
for class_label, accuracy in class_accuracy.items():
    print(f"Class {class_label}: Accuracy = {accuracy:.2f}")

# ç‰¹ç•°åº¦ã¨å½é™½æ€§ç‡
specificity = {}
false_positive_rate = {}
for i in range(conf_matrix.shape[0]):
    tn = np.sum(np.delete(np.delete(conf_matrix, i, axis=0), i, axis=1))
    fp = np.sum(np.delete(conf_matrix[i, :], i))
    specificity[i] = tn / (tn + fp)
    false_positive_rate[i] = fp / (fp + tn)

print("\nğŸ” Specificity per class:")
print(specificity)
print("\nğŸš« False Positive Rate per class:")
print(false_positive_rate)

# ROC AUCæ›²ç·š
plt.figure(figsize=(10, 8))
for i in range(np.max(y_test)+1):
    fpr, tpr, _ = roc_curve(y_test == i, y_pred[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC AUC Curves')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

model.evaluate(X_test,y_test)

np.argmax(model.predict(X_test))

"""***â€»å·¥ç¨‹ã®ç¢ºèªâ†“***"""

